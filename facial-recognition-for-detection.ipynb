{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-08T13:59:20.779282Z","iopub.execute_input":"2022-09-08T13:59:20.780358Z","iopub.status.idle":"2022-09-08T14:02:12.742732Z","shell.execute_reply.started":"2022-09-08T13:59:20.780248Z","shell.execute_reply":"2022-09-08T14:02:12.741646Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Import Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport cv2    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import f1_score\n\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\nfrom keras import optimizers\nfrom keras.models import Sequential, Model \nfrom keras.layers import Dropout, Flatten, Dense\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\nfrom keras.utils import np_utils\nfrom tensorflow.keras.optimizers import SGD\nfrom keras import regularizers\nfrom keras.layers import Conv2D, Flatten, MaxPooling1D, BatchNormalization\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom IPython.core.display import display, HTML\nfrom PIL import Image\nfrom io import BytesIO\nimport os\nimport base64\nfrom keras.applications.xception import Xception\nplt.style.use('ggplot')\n\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:12.744785Z","iopub.execute_input":"2022-09-08T14:02:12.745530Z","iopub.status.idle":"2022-09-08T14:02:18.728272Z","shell.execute_reply.started":"2022-09-08T14:02:12.745490Z","shell.execute_reply":"2022-09-08T14:02:18.727257Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:18.729613Z","iopub.execute_input":"2022-09-08T14:02:18.730553Z","iopub.status.idle":"2022-09-08T14:02:18.739214Z","shell.execute_reply.started":"2022-09-08T14:02:18.730512Z","shell.execute_reply":"2022-09-08T14:02:18.737366Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Get the version of Tensorflow","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:18.743897Z","iopub.execute_input":"2022-09-08T14:02:18.744729Z","iopub.status.idle":"2022-09-08T14:02:18.754757Z","shell.execute_reply.started":"2022-09-08T14:02:18.744700Z","shell.execute_reply":"2022-09-08T14:02:18.753883Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nprint(tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:18.755994Z","iopub.execute_input":"2022-09-08T14:02:18.758095Z","iopub.status.idle":"2022-09-08T14:02:18.767236Z","shell.execute_reply.started":"2022-09-08T14:02:18.758049Z","shell.execute_reply":"2022-09-08T14:02:18.766145Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Data Exploration\nWe will be using the CelebA Dataset, which includes images of 178 x 218 px. Below is an example of how the pictures looks like.","metadata":{}},{"cell_type":"code","source":"# set variables \nmain_folder = ''\nimages_folder = main_folder + '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/'\n\nEXAMPLE_PIC = '/kaggle/input/celeba-dataset/img_align_celeba/img_align_celeba/000001.jpg'\n\nTRAINING_SAMPLES = 5000\nVALIDATION_SAMPLES = 2000\nTEST_SAMPLES = 500\nIMG_WIDTH = 178\nIMG_HEIGHT = 218\nBATCH_SIZE=32","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:18.768821Z","iopub.execute_input":"2022-09-08T14:02:18.769373Z","iopub.status.idle":"2022-09-08T14:02:18.777513Z","shell.execute_reply.started":"2022-09-08T14:02:18.769336Z","shell.execute_reply":"2022-09-08T14:02:18.776512Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# import the data set that include the attribute for each picture\ndf_attr = pd.read_csv(\"/kaggle/input/celeba-dataset/list_attr_celeba.csv\")\ndf_attr.set_index('image_id', inplace=True)\ndf_attr.replace(to_replace=-1, value=0, inplace=True) #replace -1 by 0\ndf_attr.shape","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:18.779490Z","iopub.execute_input":"2022-09-08T14:02:18.780322Z","iopub.status.idle":"2022-09-08T14:02:19.637794Z","shell.execute_reply.started":"2022-09-08T14:02:18.780285Z","shell.execute_reply":"2022-09-08T14:02:19.636769Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# List of available attributes\nfor i, j in enumerate(df_attr.columns):\n    print(i, j)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:19.639099Z","iopub.execute_input":"2022-09-08T14:02:19.639698Z","iopub.status.idle":"2022-09-08T14:02:19.646391Z","shell.execute_reply.started":"2022-09-08T14:02:19.639659Z","shell.execute_reply":"2022-09-08T14:02:19.645401Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# plot picture and attributes\nimg = load_img(EXAMPLE_PIC)\nplt.grid(False)\nplt.imshow(img)\ndf_attr.loc[EXAMPLE_PIC.split('/')[-1]][['Smiling','Male','Young']] #some attributes","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:19.647748Z","iopub.execute_input":"2022-09-08T14:02:19.648886Z","iopub.status.idle":"2022-09-08T14:02:19.931848Z","shell.execute_reply.started":"2022-09-08T14:02:19.648847Z","shell.execute_reply":"2022-09-08T14:02:19.930790Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Training, Validation and Test\nThe recommended partitioning of images into training, validation, testing of the data set is:\n\n1-162770 are training 162771-182637 are validation 182638-202599 are testing The partition is in file list_eval_partition.csv\n\nDue time execution, by now we will be using a reduced number of images:\n\nTraining 5000 images Validation 2000 images Test 500 Images","metadata":{}},{"cell_type":"code","source":"df_partition = pd.read_csv('../input/celeba-dataset/list_eval_partition.csv')\ndf_partition.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:19.935965Z","iopub.execute_input":"2022-09-08T14:02:19.936266Z","iopub.status.idle":"2022-09-08T14:02:20.086766Z","shell.execute_reply.started":"2022-09-08T14:02:19.936239Z","shell.execute_reply":"2022-09-08T14:02:20.085748Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df_partition['partition'].value_counts().sort_index()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:20.088511Z","iopub.execute_input":"2022-09-08T14:02:20.089155Z","iopub.status.idle":"2022-09-08T14:02:20.101760Z","shell.execute_reply.started":"2022-09-08T14:02:20.089117Z","shell.execute_reply":"2022-09-08T14:02:20.100615Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"# Join the partition and the attributes in the same data frame","metadata":{}},{"cell_type":"code","source":"# join the partition with the attributes\ndf_partition.set_index('image_id', inplace=True)\ndf_par_attr = df_partition.join(df_attr['Male'], how='inner')\ndf_par_attr = df_par_attr.join(df_attr['Smiling'], how='inner')\ndf_par_attr = df_par_attr.join(df_attr['Young'], how='inner')\ndf_par_attr.head()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:20.103497Z","iopub.execute_input":"2022-09-08T14:02:20.103981Z","iopub.status.idle":"2022-09-08T14:02:20.226803Z","shell.execute_reply.started":"2022-09-08T14:02:20.103942Z","shell.execute_reply":"2022-09-08T14:02:20.225341Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def load_reshape_img(fname):\n    img = load_img(fname)\n    x = img_to_array(img)/255.\n    x = x.reshape((1,) + x.shape)\n\n    return x\n\n\ndef generate_df(partition, attr, num_samples):\n    \n    df_ = df_par_attr[(df_par_attr['partition'] == partition) \n                           & (df_par_attr[attr] == 0)].sample(int(num_samples/2))\n    df_ = pd.concat([df_,\n                      df_par_attr[(df_par_attr['partition'] == partition) \n                                  & (df_par_attr[attr] == 1)].sample(int(num_samples/2))])\n\n\n    if partition != 2:\n        x_ = np.array([load_reshape_img(images_folder + fname) for fname in df_.index])\n        x_ = x_.reshape(x_.shape[0], 218, 178, 3)\n        y_ = np_utils.to_categorical(df_[attr],2)\n\n    else:\n        x_ = []\n        y_ = []\n\n        for index, target in df_.iterrows():\n            im = cv2.imread(images_folder + index)\n            im = cv2.resize(cv2.cvtColor(im, cv2.COLOR_BGR2RGB), (IMG_WIDTH, IMG_HEIGHT)).astype(np.float32) / 255.0\n            im = np.expand_dims(im, axis =0)\n            x_.append(im)\n            y_.append(target[attr])\n\n    return x_, y_","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:20.228614Z","iopub.execute_input":"2022-09-08T14:02:20.229484Z","iopub.status.idle":"2022-09-08T14:02:20.239965Z","shell.execute_reply.started":"2022-09-08T14:02:20.229453Z","shell.execute_reply":"2022-09-08T14:02:20.238852Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation\nThis is how an image will look like after data augmentation (based in the giving parameters below).","metadata":{}},{"cell_type":"code","source":"# Generate image generator for data augmentation\ndatagen =  ImageDataGenerator(\n  #preprocessing_function=preprocess_input,\n  rotation_range=30,\n  width_shift_range=0.2,\n  height_shift_range=0.2,\n  shear_range=0.2,\n  zoom_range=0.2,\n  horizontal_flip=True\n)\n\n# load one image and reshape\nimg = load_img(EXAMPLE_PIC)\nx = img_to_array(img)/255.\nx = x.reshape((1,) + x.shape)\n\n# plot 10 augmented images of the loaded iamge\nplt.figure(figsize=(20,10))\nplt.suptitle('Data Augmentation', fontsize=28)\n\ni = 0\nfor batch in datagen.flow(x, batch_size=1):\n    plt.subplot(3, 5, i+1)\n    plt.grid(False)\n    plt.imshow( batch.reshape(218, 178, 3))\n    \n    if i == 9:\n        break\n    i += 1\n    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:20.241724Z","iopub.execute_input":"2022-09-08T14:02:20.242567Z","iopub.status.idle":"2022-09-08T14:02:21.300823Z","shell.execute_reply.started":"2022-09-08T14:02:20.242530Z","shell.execute_reply":"2022-09-08T14:02:21.299804Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The result is a new set of images with modifications from the original one, that allows to the model to learn from these variations in order to take this kind of images during the learning process and predict better never seen images.","metadata":{}},{"cell_type":"markdown","source":"# Step 4: Build the Model\n## 4.1. Set the Model","metadata":{}},{"cell_type":"code","source":"x_train, y_train = generate_df(0, 'Male', TRAINING_SAMPLES)\nx_valid, y_valid = generate_df(1, 'Male', VALIDATION_SAMPLES)\n\ntrain_datagen =  ImageDataGenerator(\n  preprocessing_function=preprocess_input,\n  rotation_range=30,\n  width_shift_range=0.2,\n  height_shift_range=0.2,\n  shear_range=0.2,\n  zoom_range=0.2,\n  horizontal_flip=True,\n)\n\ntrain_generator = train_datagen.flow(\nx_train, y_train,\nbatch_size=BATCH_SIZE\n)\n\nvalid_generator = train_datagen.flow(\nx_valid, y_valid,\nbatch_size=BATCH_SIZE\n)\n\n\nepochs = 15\nlearning_rate = 0.0001\nbatch_size = 128\nweights = os.path.join('', 'weights.h5')\ncallbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)]\n\nbase_model = Xception(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False) # Average pooling reduces output dimensions\nx = base_model.output\nx = Flatten()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\nmodel = Model(inputs=base_model.input, outputs=predictions)\nmodel.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n\n# ------ TRAINING ------\nmodel.fit_generator(train_generator,steps_per_epoch=len(x_train)/batch_size,validation_data=valid_generator\n                    ,callbacks=callbacks,epochs=epochs,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:02:21.302227Z","iopub.execute_input":"2022-09-08T14:02:21.302931Z","iopub.status.idle":"2022-09-08T14:12:52.381786Z","shell.execute_reply.started":"2022-09-08T14:02:21.302878Z","shell.execute_reply":"2022-09-08T14:12:52.380549Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"x_train, y_train = generate_df(0, 'Smiling', TRAINING_SAMPLES)\nx_valid, y_valid = generate_df(1, 'Smiling', VALIDATION_SAMPLES)\n\ntrain_datagen =  ImageDataGenerator(\n  preprocessing_function=preprocess_input,\n  rotation_range=30,\n  width_shift_range=0.2,\n  height_shift_range=0.2,\n  shear_range=0.2,\n  zoom_range=0.2,\n  horizontal_flip=True,\n)\n\ntrain_generator = train_datagen.flow(\nx_train, y_train,\nbatch_size=BATCH_SIZE\n)\n\nvalid_generator = train_datagen.flow(\nx_valid, y_valid,\nbatch_size=BATCH_SIZE\n)\n\n\n\nepochs = 15\nlearning_rate = 0.0001\nbatch_size = 128\nweights = os.path.join('', 'weights.h5')\ncallbacks = [ EarlyStopping(monitor='val_loss', patience=5, verbose=0), \n              ModelCheckpoint(weights, monitor='val_loss', save_best_only=True, verbose=0),\n              ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)]\n\nbase_model = Xception(input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), include_top=False) # Average pooling reduces output dimensions\nx = base_model.output\nx = Flatten()(x)\nx = Dense(256, activation='relu')(x)\nx = Dropout(0.5)(x)\npredictions = Dense(2, activation='softmax')(x)\nmodel_smile = Model(inputs=base_model.input, outputs=predictions)    \nmodel_smile.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(lr=learning_rate), metrics=['accuracy'])\n\n# ------ TRAINING ------\nmodel_smile.fit_generator(train_generator,steps_per_epoch=len(x_train)/batch_size,validation_data=valid_generator\n                    ,callbacks=callbacks,epochs=epochs,verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:12:52.386394Z","iopub.execute_input":"2022-09-08T14:12:52.386694Z","iopub.status.idle":"2022-09-08T14:23:04.731905Z","shell.execute_reply.started":"2022-09-08T14:12:52.386668Z","shell.execute_reply":"2022-09-08T14:23:04.730957Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Model Evaluation","metadata":{}},{"cell_type":"code","source":"x_test, y_test = generate_df(2, 'Male', TEST_SAMPLES)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:04.733143Z","iopub.execute_input":"2022-09-08T14:23:04.734594Z","iopub.status.idle":"2022-09-08T14:23:07.298628Z","shell.execute_reply.started":"2022-09-08T14:23:04.734553Z","shell.execute_reply":"2022-09-08T14:23:07.296370Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"x_test_smile, y_test_smile = generate_df(2, 'Smiling', TEST_SAMPLES)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:07.304800Z","iopub.execute_input":"2022-09-08T14:23:07.306750Z","iopub.status.idle":"2022-09-08T14:23:09.493753Z","shell.execute_reply.started":"2022-09-08T14:23:07.306719Z","shell.execute_reply":"2022-09-08T14:23:09.492628Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"#x_test_young, y_test_young = generate_df(2, 'Young', TEST_SAMPLES)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:09.495255Z","iopub.execute_input":"2022-09-08T14:23:09.495609Z","iopub.status.idle":"2022-09-08T14:23:09.502901Z","shell.execute_reply.started":"2022-09-08T14:23:09.495576Z","shell.execute_reply":"2022-09-08T14:23:09.500336Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if os.path.isfile(weights):\n    model.load_weights(weights)\n    model_smile.load_weights(weights)\n    #model_young.load_weights(weights)","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:09.504327Z","iopub.execute_input":"2022-09-08T14:23:09.504763Z","iopub.status.idle":"2022-09-08T14:23:10.532650Z","shell.execute_reply.started":"2022-09-08T14:23:09.504693Z","shell.execute_reply":"2022-09-08T14:23:10.531484Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model_predictions = [np.argmax(model.predict(feature)) for feature in x_test ]","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:10.534258Z","iopub.execute_input":"2022-09-08T14:23:10.534610Z","iopub.status.idle":"2022-09-08T14:23:32.442696Z","shell.execute_reply.started":"2022-09-08T14:23:10.534574Z","shell.execute_reply":"2022-09-08T14:23:32.441693Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"model_predictions_smile = [np.argmax(model_smile.predict(feature)) for feature in x_test_smile ]","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:32.444417Z","iopub.execute_input":"2022-09-08T14:23:32.444814Z","iopub.status.idle":"2022-09-08T14:23:55.242797Z","shell.execute_reply.started":"2022-09-08T14:23:32.444779Z","shell.execute_reply":"2022-09-08T14:23:55.241816Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"#model_predictions_young = [np.argmax(model_young.predict(feature)) for feature in x_test_young ]","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:55.244441Z","iopub.execute_input":"2022-09-08T14:23:55.244795Z","iopub.status.idle":"2022-09-08T14:23:55.251773Z","shell.execute_reply.started":"2022-09-08T14:23:55.244758Z","shell.execute_reply":"2022-09-08T14:23:55.248784Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"test_accuracy = 100 * np.sum(np.array(model_predictions)==y_test) / len(model_predictions)\nprint('Model Evaluation ')\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint('f1_score:', f1_score(y_test_smile, model_predictions_smile))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:55.253582Z","iopub.execute_input":"2022-09-08T14:23:55.254654Z","iopub.status.idle":"2022-09-08T14:23:55.271741Z","shell.execute_reply.started":"2022-09-08T14:23:55.254607Z","shell.execute_reply":"2022-09-08T14:23:55.270618Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"test_accuracy = 100 * np.sum(np.array(model_predictions_smile)==y_test_smile) / len(model_predictions_smile)\nprint('Model Evaluation')\nprint('Test accuracy: %.4f%%' % test_accuracy)\nprint('f1_score:', f1_score(y_test_smile, model_predictions_smile))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:55.273120Z","iopub.execute_input":"2022-09-08T14:23:55.273639Z","iopub.status.idle":"2022-09-08T14:23:55.284323Z","shell.execute_reply.started":"2022-09-08T14:23:55.273604Z","shell.execute_reply":"2022-09-08T14:23:55.282877Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"#test_accuracy = 100 * np.sum(np.array(model_predictions_young)==y_test_young) / len(model_predictions_young)\n#print('Model Evaluation')\n#print('Test accuracy: %.4f%%' % test_accuracy)\n#print('f1_score:', f1_score(y_test_smile, model_predictions_smile))","metadata":{"execution":{"iopub.status.busy":"2022-09-08T14:23:55.285993Z","iopub.execute_input":"2022-09-08T14:23:55.286348Z","iopub.status.idle":"2022-09-08T14:23:55.294479Z","shell.execute_reply.started":"2022-09-08T14:23:55.286315Z","shell.execute_reply":"2022-09-08T14:23:55.293507Z"},"trusted":true},"execution_count":26,"outputs":[]}]}